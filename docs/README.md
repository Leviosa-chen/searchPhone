# 网站手机号码和联系人爬虫

这是一个Python爬虫脚本，用于爬取指定网站的所有页面，提取其中的手机号码和联系人信息，并导出到CSV和JSON文件。

## 功能特点

- 🕷️ 自动爬取网站所有页面
- 📱 智能识别中国手机号码（支持多种格式，避免误匹配文件名）
- 👤 提取联系人信息（注重语义，过滤无效内容）
- 📊 导出为CSV、JSON和Word格式
- 🔍 避免重复爬取同一页面
- 🚫 智能过滤文件名中的数字序列
- 🔄 自动去重：重复的手机号和联系人只保留首次出现
- 📝 详细的日志记录
- ⚡ 可配置的爬取速度限制
- 📄 Word文档导出：自动生成格式化的Word文档
- 🧹 智能文本清理：自动去除特殊字符，保留有效内容
- 🚀 无限制爬取：可选择爬取所有页面或限制页数
- ⚙️ 灵活配置：支持自定义爬取参数和模式选择

## 安装依赖

```bash
pip install -r requirements.txt
```

或者手动安装：

```bash
pip install requests beautifulsoup4 lxml
```

## 使用方法

### 基本使用

```bash
python phone_scraper.py
```

### 自定义配置

你可以修改脚本中的以下参数：

- `max_pages`: 最大爬取页数（默认200页）
- `target_url`: 目标网站URL
- 爬取间隔时间（默认1秒）

## 输出文件

脚本运行完成后会生成以下文件：

1. **phone_contacts.csv** - CSV格式的结果文件
2. **phone_contacts.json** - JSON格式的结果文件
3. **phone_contacts.docx** - Word格式的结果文件（推荐查看）
4. **scraper.log** - 详细的运行日志

## 输出格式

### CSV文件包含以下列：

- `url`: 页面URL
- `title`: 页面标题
- `phone_numbers`: 手机号码（多个用分号分隔，已去重）
- `contacts`: 联系人信息（多个用分号分隔，已去重）
- `phone_count`: 手机号码数量（去重后）
- `contact_count`: 联系人数量（去重后）
- `original_phones`: 原始手机号码（包含重复，用于对比）
- `original_contacts`: 原始联系人信息（包含重复，用于对比）

### JSON文件结构：

```json
[
  {
    "url": "页面URL",
    "title": "页面标题",
    "phone_numbers": "手机号码1; 手机号码2",
    "contacts": "联系人1; 联系人2",
    "phone_count": 2,
    "contact_count": 2,
    "original_phones": "原始手机号码（包含重复）",
    "original_contacts": "原始联系人信息（包含重复）"
  }
]
```

## 手机号码识别规则

脚本支持识别以下格式的中国手机号码，并智能过滤无效匹配：

- 标准11位：13800138000
- 带+86前缀：+86 13800138000
- 带86前缀：86 13800138000
- 带空格分隔：138 0013 8000
- 带连字符：138-0013-8000

### 智能过滤机制

- **前后数字检查**：使用负向前瞻和负向后瞻确保手机号前后不是数字
- **文件名检测**：自动识别并过滤文件名中的数字序列（如 1583983093178085133.jpg）
- **长数字过滤**：避免从长数字序列中误提取手机号
- **上下文分析**：检查手机号周围的字符，判断是否为有效联系信息

## 联系人识别规则

脚本会智能查找并提取联系人信息，注重语义准确性：

### 支持的联系人格式

- **标准格式**：联系人：张三、负责人: 李四
- **带职务格式**：联系人：王五 经理、负责人：赵六 主管
- **表格格式**：联系人 钱七、主管 孙八
- **姓名格式**：姓名：周九

### 智能验证机制

- **长度限制**：联系人长度在2-20字符之间
- **内容验证**：必须包含中文或英文字符
- **无效内容过滤**：自动排除纯数字、邮箱、手机号、座机号等
- **语义分析**：基于关键词和上下文进行智能提取

## 去重功能

### 去重策略

爬虫采用"首次出现优先"的去重策略：

- **手机号去重**：相同的手机号只在第一次出现时记录，后续重复出现时自动过滤
- **联系人去重**：相同的联系人信息只在第一次出现时记录，后续重复出现时自动过滤
- **全局去重**：在整个爬取过程中，所有页面共享去重状态

### 去重优势

1. **避免重复数据**：确保最终结果中每个手机号和联系人只出现一次
2. **保留首次信息**：优先保留在网站中首次出现的信息，通常更准确
3. **减少数据冗余**：降低后期数据处理的复杂度
4. **提高数据质量**：避免因重复信息导致的统计偏差

### 去重统计

爬取过程中会显示去重统计信息：

```
✓ 找到 2 个新手机号, 1 个新联系人
   新手机号: 13800138000, 13900139000
   新联系人: 张三
   (去重: 1 个重复手机号, 0 个重复联系人)
```

### 原始数据保留

为了便于对比和分析，系统会同时保存：
- **去重后数据**：用于最终结果展示
- **原始数据**：包含重复信息，用于数据对比和调试

## Word导出功能

### 文档格式

爬虫会自动生成格式化的Word文档，包含以下内容：

- **文档标题**：使用网站标题作为文档主标题
- **爬取信息**：爬取时间、页面数量、统计信息
- **内容列表**：按页面组织的手机号和联系人信息
- **来源标注**：每个信息的来源页面URL

### 文档特点

1. **自动格式化**：标题层级、字体样式、段落布局自动处理
2. **内容清理**：自动去除特殊字符，保留有效内容
3. **智能过滤**：只导出包含联系信息的页面
4. **专业排版**：适合正式报告和文档使用

### 文本清理规则

系统会自动清理以下特殊字符：
- HTML标签：`<div>`, `<span>`, `<br>` 等
- 特殊符号：`@`, `&`, `#`, `%` 等
- 多余空格：多个连续空格合并为单个
- 保留内容：中文、英文、数字、空格、冒号、括号

### 使用建议

- **主要查看**：推荐使用Word文档查看结果
- **数据分析**：CSV文件适合Excel处理
- **程序处理**：JSON文件适合程序读取

## 爬取模式

### 无限制爬取模式（默认）

爬虫默认设置为无限制爬取模式，将爬取所有可访问的页面：

- **爬取范围**：网站内所有可访问的页面
- **安全机制**：内置10000页的安全限制，防止无限爬取
- **进度显示**：每10页显示一次爬取进度
- **智能停止**：当没有新链接可爬取时自动停止

### 限制爬取模式

适合快速测试和有限范围爬取：

- **页数限制**：最多爬取指定数量的页面
- **快速完成**：适合快速获取部分数据
- **资源控制**：避免长时间运行占用资源

### 自定义配置

通过编辑 `scraper_config.py` 文件可以自定义各种参数：

```python
CRAWLER_CONFIG = {
    'limit_pages': False,      # 是否限制页数
    'max_pages': 1000,        # 最大爬取页数
    'safety_limit': 10000,    # 安全限制
    'request_delay': 0.5,     # 请求间隔时间
    'show_progress': True,    # 是否显示进度
}
```

### 启动方式

1. **快速启动**（推荐）：
   ```bash
   python quick_start.py
   ```

2. **直接启动无限制模式**：
   ```bash
   python simple_scraper.py
   ```

3. **菜单式启动**：
   ```bash
   python run_scraper.py
   ```

### 安全特性

- **重复检测**：自动跳过已访问的页面
- **链接验证**：只爬取同域名的有效链接
- **进度监控**：实时显示爬取进度和统计信息
- **中断恢复**：支持Ctrl+C中断，已爬取数据自动保存

## 注意事项

1. **遵守网站robots.txt规则**
2. **合理控制爬取速度，避免对目标网站造成压力**
3. **仅用于合法用途，请勿用于非法活动**
4. **爬取过程中可以按Ctrl+C中断，已爬取的结果会被保存**

## 错误处理

- 如果爬取过程中发生错误，已爬取的结果会被保存到 `phone_contacts_error.csv`
- 如果用户中断爬取，结果会被保存到 `phone_contacts_partial.csv`
- 所有操作都会记录在 `scraper.log` 文件中

## 技术特点

- 使用 `requests` 进行HTTP请求
- 使用 `BeautifulSoup` 解析HTML内容
- 使用正则表达式提取手机号码
- 智能URL去重和域名验证
- 支持中文编码处理
- 异常处理和错误恢复

## 许可证

本项目仅供学习和研究使用，请遵守相关法律法规和网站使用条款。 