#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆç½‘ç«™æ‰‹æœºå·ç å’Œè”ç³»äººçˆ¬è™«
ä¸“é—¨ç”¨äºçˆ¬å– https://www.schdri.com ç½‘ç«™
"""

import requests
from bs4 import BeautifulSoup
import re
import csv
import time
import json
from urllib.parse import urljoin, urlparse
from docx import Document
from docx.shared import Inches
import os

class SimplePhoneScraper:
    def __init__(self):
        self.base_url = "https://www.schdri.com"
        self.start_url = "https://www.schdri.com/go.htm?k=zhong_dian_gong_cheng&url=cheng_guo_zhan_shi/zhong_dian_gong_cheng"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.visited = set()
        self.results = []
        # ç”¨äºè·Ÿè¸ªå·²å‡ºç°çš„æ‰‹æœºå·ï¼Œç¡®ä¿ä¸é‡å¤
        self.seen_phones = set()
        # é¡µé¢å±‚çº§è·Ÿè¸ª
        self.page_levels = {}  # è®°å½•æ¯ä¸ªé¡µé¢çš„å±‚çº§
        
    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬ï¼Œå»æ‰ç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        
        # å»æ‰ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€å†’å·ã€æ‹¬å·
        cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\sï¼š:()ï¼ˆï¼‰]', '', text)
        # å»æ‰å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def extract_phones(self, text):
        """æå–æ‰‹æœºå·ç  - ä¼˜åŒ–ç‰ˆï¼Œç¡®ä¿å‰åä¸æ˜¯æ•°å­—"""
        # ä¸­å›½æ‰‹æœºå·ç æ¨¡å¼ - ä½¿ç”¨è´Ÿå‘å‰ç»å’Œè´Ÿå‘åç»ç¡®ä¿å‰åä¸æ˜¯æ•°å­—
        patterns = [
            r'(?<!\d)1[3-9]\d{9}(?!\d)',  # æ ‡å‡†11ä½ï¼Œå‰åä¸èƒ½æ˜¯æ•°å­—
            r'(?<!\d)\+86\s*1[3-9]\d{9}(?!\d)',  # å¸¦+86å‰ç¼€
            r'(?<!\d)86\s*1[3-9]\d{9}(?!\d)',  # å¸¦86å‰ç¼€
            r'(?<!\d)1[3-9]\d{2}\s*\d{4}\s*\d{4}(?!\d)',  # å¸¦ç©ºæ ¼çš„æ‰‹æœºå·
            r'(?<!\d)1[3-9]\d{2}-\d{4}-\d{4}(?!\d)',  # å¸¦è¿å­—ç¬¦çš„æ‰‹æœºå·
        ]
        
        phones = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # æ¸…ç†å·ç æ ¼å¼
                clean_phone = re.sub(r'[\s\+\-]', '', match)
                if len(clean_phone) == 11 and clean_phone.startswith('1'):
                    # é¢å¤–éªŒè¯ï¼šç¡®ä¿ä¸æ˜¯æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                    if not self._is_filename_part(clean_phone, text):
                        phones.append(clean_phone)
        
        return list(set(phones))  # å»é‡
    
    def _is_filename_part(self, phone, text):
        """æ£€æŸ¥æ‰‹æœºå·æ˜¯å¦æ˜¯æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†"""
        # æŸ¥æ‰¾æ‰‹æœºå·åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        phone_pos = text.find(phone)
        if phone_pos == -1:
            return False
        
        # æ£€æŸ¥å‰åå­—ç¬¦ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯æ–‡ä»¶å
        start_pos = max(0, phone_pos - 10)
        end_pos = min(len(text), phone_pos + 21)
        context = text[start_pos:end_pos]
        
        # å¦‚æœåŒ…å«å¸¸è§æ–‡ä»¶æ‰©å±•åï¼Œå¯èƒ½æ˜¯æ–‡ä»¶å
        file_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.pdf', '.doc', '.docx', '.xls', '.xlsx']
        for ext in file_extensions:
            if ext in context.lower():
                return True
        
        # å¦‚æœå‰åéƒ½æ˜¯æ•°å­—ï¼Œå¯èƒ½æ˜¯é•¿æ•°å­—çš„ä¸€éƒ¨åˆ†
        if phone_pos > 0 and phone_pos + 11 < len(text):
            prev_char = text[phone_pos - 1]
            next_char = text[phone_pos + 11]
            if prev_char.isdigit() and next_char.isdigit():
                return True
        
        return False
    
    def extract_contacts(self, text):
        """æå–è”ç³»äºº - ä¼˜åŒ–ç‰ˆï¼Œæ³¨æ„å‰åè¯­ä¹‰"""
        # æ”¹è¿›çš„è”ç³»äººæ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´æ³¨é‡è¯­ä¹‰
        contact_patterns = [
            # æ ‡å‡†æ ¼å¼ï¼šå…³é”®è¯: è”ç³»äºº
            r'(?:è”ç³»äºº|è´Ÿè´£äºº|ç»ç†|ä¸»ç®¡|ä¸»ä»»|æ€»ç›‘|å§“å|åå­—)[ï¼š:]\s*([^\n\r]{1,15})',
            
            # å¸¦èŒåŠ¡çš„æ ¼å¼ï¼šè”ç³»äººï¼šå¼ ä¸‰ ç»ç†
            r'(?:è”ç³»äºº|è´Ÿè´£äºº|ç»ç†|ä¸»ç®¡|ä¸»ä»»|æ€»ç›‘)[ï¼š:]\s*([^\n\r]{1,20})',
            
            # è¡¨æ ¼æ ¼å¼ï¼šè”ç³»äºº å¼ ä¸‰
            r'(?:è”ç³»äºº|è´Ÿè´£äºº|ç»ç†|ä¸»ç®¡|ä¸»ä»»|æ€»ç›‘)\s+([^\n\r]{1,15})',
            
            # å§“åæ ¼å¼ï¼šå§“åï¼šå¼ ä¸‰
            r'å§“å[ï¼š:]\s*([^\n\r]{1,15})',
        ]
        
        contacts = []
        for pattern in contact_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                contact = match.strip()
                if self._is_valid_contact(contact):
                    contacts.append(contact)
        
        return list(set(contacts))  # å»é‡
    
    def _is_valid_contact(self, contact):
        """éªŒè¯è”ç³»äººä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ"""
        if not contact:
            return False
        
        # é•¿åº¦æ£€æŸ¥
        if len(contact) < 2 or len(contact) > 20:
            return False
        
        # æ’é™¤æ˜æ˜¾æ— æ•ˆçš„å†…å®¹
        invalid_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$',  # é‚®ç®±
            r'^1[3-9]\d{9}$',  # æ‰‹æœºå·
            r'^\d{3,4}-\d{7,8}$',  # åº§æœºå·
            r'^[^\u4e00-\u9fa5a-zA-Z]*$',  # ä¸åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, contact):
                return False
        
        # å¿…é¡»åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡
        if not re.search(r'[\u4e00-\u9fa5a-zA-Z]', contact):
            return False
        
        return True
    
    def get_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def find_links(self, html, current_url):
        """æŸ¥æ‰¾é¡µé¢ä¸­çš„é“¾æ¥"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            full_url = urljoin(current_url, href)
            
            # åªä¿ç•™åŒåŸŸåçš„é“¾æ¥
            if urlparse(full_url).netloc == 'www.schdri.com':
                links.append(full_url)
        
        return links
    
    def crawl(self, max_pages=1000, max_level=6):
        """å¼€å§‹çˆ¬å– - é™åˆ¶1000é¡µï¼Œæœ€å¤š6çº§é¡µé¢"""
        urls_to_visit = [(self.start_url, 0)]  # (url, level)
        page_count = 0
        self.site_title = "æœªçŸ¥ç½‘ç«™"  # é»˜è®¤ç½‘ç«™æ ‡é¢˜
        
        print(f"å¼€å§‹çˆ¬å–ç½‘ç«™: {self.start_url}")
        print("=" * 60)
        print(f"çˆ¬å–é™åˆ¶: æœ€å¤š{max_pages}é¡µï¼Œæœ€å¤š{max_level}çº§é¡µé¢")
        print("çˆ¬å–å†…å®¹: ä»…æ‰‹æœºå·ï¼ˆä¸çˆ¬å–è”ç³»äººï¼‰")
        print("å¦‚éœ€åœæ­¢ï¼Œè¯·æŒ‰ Ctrl+C")
        print("=" * 60)
        
        # è·å–ç½‘ç«™æ ‡é¢˜
        first_page = self.get_page(self.start_url)
        if first_page:
            soup = BeautifulSoup(first_page, 'html.parser')
            title = soup.find('title')
            if title:
                self.site_title = self.clean_text(title.get_text().strip())
                print(f"ç½‘ç«™æ ‡é¢˜: {self.site_title}")
        
        while urls_to_visit and page_count < max_pages:
            current_url, current_level = urls_to_visit.pop(0)
            
            if current_url in self.visited:
                continue
            
            # æ£€æŸ¥é¡µé¢å±‚çº§
            if current_level > max_level:
                print(f"âš ï¸  è·³è¿‡é¡µé¢ {current_url} - å±‚çº§ {current_level} è¶…è¿‡é™åˆ¶ {max_level}")
                continue
            
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page_count + 1} é¡µ (å±‚çº§ {current_level}): {current_url}")
            
            # è·å–é¡µé¢å†…å®¹
            html = self.get_page(current_url)
            if not html:
                continue
            
            self.visited.add(current_url)
            self.page_levels[current_url] = current_level
            page_count += 1
            
            # åªæå–æ‰‹æœºå·ï¼Œä¸æå–è”ç³»äºº
            phones = self.extract_phones(html)
            
            # å»é‡å¤„ç†ï¼šåªä¿ç•™é¦–æ¬¡å‡ºç°çš„æ‰‹æœºå·
            unique_phones = []
            for phone in phones:
                if phone not in self.seen_phones:
                    unique_phones.append(phone)
                    self.seen_phones.add(phone)
            
            # è·å–é¡µé¢æ ‡é¢˜
            soup = BeautifulSoup(html, 'html.parser')
            title = soup.find('title')
            page_title = self.clean_text(title.get_text().strip()) if title else "æ— æ ‡é¢˜"
            
            # è®°å½•ç»“æœ - åªè®°å½•æœ‰æ‰‹æœºå·çš„é¡µé¢
            if unique_phones:
                result = {
                    'url': current_url,
                    'title': page_title,
                    'phones': unique_phones,
                    'phone_count': len(unique_phones),
                    'level': current_level,
                    'original_phones': phones  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äºå¯¹æ¯”
                }
                self.results.append(result)
                
                print(f"  âœ“ æ‰¾åˆ° {len(unique_phones)} ä¸ªæ–°æ‰‹æœºå·")
                if unique_phones:
                    print(f"     æ–°æ‰‹æœºå·: {', '.join(unique_phones)}")
                
                # å¦‚æœæœ‰é‡å¤æ•°æ®ï¼Œæ˜¾ç¤ºå»é‡ä¿¡æ¯
                duplicate_phones = len(phones) - len(unique_phones)
                if duplicate_phones > 0:
                    print(f"     (å»é‡: {duplicate_phones} ä¸ªé‡å¤æ‰‹æœºå·)")
            else:
                print(f"  - æœªæ‰¾åˆ°æ–°çš„æ‰‹æœºå·")
            
            # æŸ¥æ‰¾æ–°é“¾æ¥ï¼Œä½†é™åˆ¶å±‚çº§
            if current_level < max_level:
                new_links = self.find_links(html, current_url)
                for link in new_links:
                    if link not in self.visited and not any(link == url for url, _ in urls_to_visit):
                        urls_to_visit.append((link, current_level + 1))
            
            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            if page_count % 10 == 0:
                print(f"\nğŸ“Š çˆ¬å–è¿›åº¦: å·²çˆ¬å– {page_count}/{max_pages} é¡µï¼Œå¾…çˆ¬å– {len(urls_to_visit)} é¡µ")
                print(f"ğŸ“± å·²æ‰¾åˆ° {len(self.seen_phones)} ä¸ªæ‰‹æœºå·")
                print(f"ğŸŒ å½“å‰æœ€é«˜å±‚çº§: {max(self.page_levels.values()) if self.page_levels else 0}")
            
            # æ§åˆ¶çˆ¬å–é€Ÿåº¦
            time.sleep(0.5)
        
        print(f"\nçˆ¬å–å®Œæˆï¼å…±çˆ¬å– {page_count} é¡µ")
        print(f"æ‰¾åˆ° {len(self.results)} é¡µåŒ…å«æ‰‹æœºå·")
        print(f"æ€»è®¡ {len(self.seen_phones)} ä¸ªæ‰‹æœºå·")
        print(f"æœ€é«˜é¡µé¢å±‚çº§: {max(self.page_levels.values()) if self.page_levels else 0}")
        
        # æ˜¾ç¤ºå±‚çº§ç»Ÿè®¡
        level_stats = {}
        for level in self.page_levels.values():
            level_stats[level] = level_stats.get(level, 0) + 1
        
        print(f"\né¡µé¢å±‚çº§åˆ†å¸ƒ:")
        for level in sorted(level_stats.keys()):
            print(f"  å±‚çº§ {level}: {level_stats[level]} é¡µ")
    
    def export_docx(self, filename='phone_contacts.docx'):
        """å¯¼å‡ºåˆ°Wordæ–‡æ¡£ - ä»…æ‰‹æœºå·"""
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‰‹æœºå·ä¿¡æ¯")
            return
        
        try:
            # åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®æ ‡é¢˜
            title = doc.add_heading(self.site_title, 0)
            title.alignment = 1  # å±…ä¸­å¯¹é½
            
            # æ·»åŠ è¯´æ˜
            doc.add_paragraph(f"çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            doc.add_paragraph(f"çˆ¬å–é¡µé¢æ•°: {len(self.results)}")
            doc.add_paragraph(f"æ€»è®¡æ‰‹æœºå·: {len(self.seen_phones)} ä¸ª")
            doc.add_paragraph(f"æœ€é«˜é¡µé¢å±‚çº§: {max(self.page_levels.values()) if self.page_levels else 0}")
            
            # æ·»åŠ åˆ†éš”çº¿
            doc.add_paragraph("=" * 50)
            
            # æ·»åŠ å†…å®¹
            for i, result in enumerate(self.results, 1):
                # é¡µé¢æ ‡é¢˜
                page_heading = doc.add_heading(f"{i}. {result['title']}", level=1)
                
                # æ‰‹æœºå·ä¿¡æ¯
                if result['phones']:
                    phone_para = doc.add_paragraph()
                    phone_para.add_run("æ‰‹æœºå·: ").bold = True
                    phone_para.add_run(", ".join(result['phones']))
                
                # é¡µé¢å±‚çº§ä¿¡æ¯
                level_para = doc.add_paragraph()
                level_para.add_run("é¡µé¢å±‚çº§: ").bold = True
                level_para.add_run(f"ç¬¬ {result['level']} çº§")
                
                # é¡µé¢URLï¼ˆå°å­—ï¼‰
                url_para = doc.add_paragraph()
                url_run = url_para.add_run(f"æ¥æº: {result['url']}")
                url_run.font.size = doc.styles['Normal'].font.size
                url_run.font.color.rgb = None  # é»˜è®¤é¢œè‰²
                
                # æ·»åŠ åˆ†éš”çº¿
                doc.add_paragraph("-" * 30)
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(filename)
            print(f"\nç»“æœå·²å¯¼å‡ºåˆ°Wordæ–‡æ¡£: {filename}")
            
        except Exception as e:
            print(f"å¯¼å‡ºWordæ–‡æ¡£å¤±è´¥: {e}")
    
    def export_csv(self, filename='phone_contacts.csv'):
        """å¯¼å‡ºåˆ°CSV - ä»…æ‰‹æœºå·"""
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‰‹æœºå·ä¿¡æ¯")
            return
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['é¡µé¢æ ‡é¢˜', 'URL', 'æ‰‹æœºå·ç ', 'æ‰‹æœºå·æ•°é‡', 'é¡µé¢å±‚çº§'])
                
                for result in self.results:
                    writer.writerow([
                        result['title'],
                        result['url'],
                        '; '.join(result['phones']),
                        result['phone_count'],
                        result['level']
                    ])
            
            print(f"\nç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
            
            # ç»Ÿè®¡æ€»æ•°
            total_phones = sum(r['phone_count'] for r in self.results)
            print(f"æ€»è®¡: {total_phones} ä¸ªæ‰‹æœºå·")
            
        except Exception as e:
            print(f"å¯¼å‡ºCSVå¤±è´¥: {e}")
    
    def export_json(self, filename='phone_contacts.json'):
        """å¯¼å‡ºåˆ°JSON"""
        if not self.results:
            return
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
        except Exception as e:
            print(f"å¯¼å‡ºJSONå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    scraper = SimplePhoneScraper()
    
    try:
        # å¼€å§‹çˆ¬å– - é™åˆ¶1000é¡µï¼Œæœ€å¤š6çº§é¡µé¢
        scraper.crawl(max_pages=1000, max_level=6)
        
        # å¯¼å‡ºç»“æœ
        scraper.export_csv()
        scraper.export_json()
        scraper.export_docx() # æ–°å¢çš„å¯¼å‡ºæ–¹æ³•
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        if scraper.results:
            print("\n" + "=" * 60)
            print("çˆ¬å–ç»“æœæ‘˜è¦:")
            print("=" * 60)
            
            for i, result in enumerate(scraper.results[:5], 1):
                print(f"\n{i}. {result['title']}")
                print(f"   URL: {result['url']}")
                if result['phones']:
                    print(f"   æ‰‹æœºå·: {', '.join(result['phones'])}")
                if result['contacts']:
                    print(f"   è”ç³»äºº: {', '.join(result['contacts'])}")
            
            if len(scraper.results) > 5:
                print(f"\n... è¿˜æœ‰ {len(scraper.results) - 5} é¡µç»“æœ")
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­çˆ¬å–")
        if scraper.results:
            scraper.export_csv('phone_contacts_partial.csv')
            scraper.export_json('phone_contacts_partial.json')
            print("å·²å¯¼å‡ºéƒ¨åˆ†ç»“æœ")
    except Exception as e:
        print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if scraper.results:
            scraper.export_csv('phone_contacts_error.csv')
            scraper.export_json('phone_contacts_error.json')

if __name__ == "__main__":
    main() 